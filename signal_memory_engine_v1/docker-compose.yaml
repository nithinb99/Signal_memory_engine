services:
  mlflow:
    image: python:3.10-slim
    container_name: mlflow
    command: >
      sh -lc "
        apt-get update &&
        apt-get install -y --no-install-recommends wget &&
        rm -rf /var/lib/apt/lists/* &&
        python -m pip install --upgrade pip &&
        pip install mlflow &&
        mkdir -p /mlruns &&
        mlflow server
          --backend-store-uri sqlite:///mlflow.db
          --default-artifact-root /mlruns
          --host 0.0.0.0
          --port 5000
      "
    ports: ["5000:5000"]
    volumes: ["./mlruns:/mlruns"]
    healthcheck:
      test: ["CMD", "wget", "-qO-", "http://localhost:5000"]
      interval: 5s
      timeout: 3s
      retries: 20

  backend:
    build:
      context: .
      dockerfile: dockerfile.backend
    container_name: backend
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - PINECONE_API_KEY=${PINECONE_API_KEY}
      - PINECONE_INDEX=${PINECONE_INDEX}
      - PINECONE_ENVIRONMENT=${PINECONE_ENVIRONMENT:-us-east-1}
      - MLFLOW_TRACKING_URI=http://mlflow:5000
      - MLFLOW_EXPERIMENT_NAME=signal-memory-engine
      - OPENAI_MODEL=${OPENAI_MODEL:-gpt-4o-mini}
      - PORT=8000
    depends_on:
      mlflow:
        condition: service_healthy
    ports: ["8000:8000"]

  ui:
    build:
      context: .
      dockerfile: dockerfile.ui
    container_name: ui
    environment:
      - BACKEND_URL=http://backend:8000
      - MLFLOW_TRACKING_URI=http://mlflow:5000
    depends_on:
      - backend
      - mlflow
    ports: ["8501:8501"]
