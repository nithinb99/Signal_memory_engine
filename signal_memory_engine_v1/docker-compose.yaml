services:
  mlflow:
    image: python:3.11-slim
    container_name: mlflow
    command: >
      sh -c "python -m pip install --upgrade pip &&
             pip install mlflow &&
             mkdir -p /mlruns &&
             mlflow server
               --backend-store-uri sqlite:///mlflow.db
               --default-artifact-root /mlruns
               --host 0.0.0.0
               --port 5000"
    ports:
      - "5000:5000"
    volumes:
      - ./mlruns:/mlruns
    healthcheck:
      test: ["CMD", "wget", "-qO-", "http://localhost:5000"]
      interval: 5s
      timeout: 3s
      retries: 20

  backend:
    build:
      context: .
      dockerfile: dockerfile.backend
    container_name: backend
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - PINECONE_API_KEY=${PINECONE_API_KEY}
      - PINECONE_INDEX=${PINECONE_INDEX}
      - PINECONE_ENVIRONMENT=${PINECONE_ENVIRONMENT:-us-east-1}
      - MLFLOW_TRACKING_URI=http://mlflow:5000
      - MLFLOW_EXPERIMENT_NAME=SignalMemoryEngine
      - OPENAI_MODEL=${OPENAI_MODEL:-gpt-4o-mini}
    depends_on:
      mlflow:
        condition: service_healthy
    ports:
      - "8000:8000"

  ui:
    build:
      context: .
      dockerfile: dockerfile.ui
    container_name: ui
    environment:
      - BACKEND_URL=http://backend:8000
      - MLFLOW_TRACKING_URI=http://mlflow:5000
    depends_on:
      - backend
      - mlflow
    ports:
      - "8501:8501"
