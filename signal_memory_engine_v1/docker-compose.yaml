services:
  mlflow:
    build:
      context: .
      dockerfile: dockerfile.mlflow
    container_name: mlflow
    ports:
      - "5000:5000"
    volumes:
      - ./mlruns:/mlruns
    healthcheck:
      test: ["CMD", "wget", "-qO-", "http://localhost:5000"]
      interval: 5s
      timeout: 3s
      retries: 20

  backend:
    build:
      context: .
      dockerfile: dockerfile.backend
    container_name: backend
    environment:
      OPENAI_API_KEY: ${OPENAI_API_KEY}
      PINECONE_API_KEY: ${PINECONE_API_KEY}
      PINECONE_INDEX: ${PINECONE_INDEX}
      PINECONE_ENVIRONMENT: ${PINECONE_ENVIRONMENT:-us-east-1}
      MLFLOW_TRACKING_URI: http://mlflow:5000
      MLFLOW_EXPERIMENT_NAME: signal-memory-engine
      OPENAI_MODEL: ${OPENAI_MODEL:-gpt-4o-mini}
      PORT: "8000"
    depends_on:
      mlflow:
        condition: service_healthy
    ports:
      - "8000:8000"

  ui:
    build:
      context: .
      dockerfile: dockerfile.ui
    container_name: ui
    environment:
      BACKEND_URL: http://backend:8000
      MLFLOW_TRACKING_URI: http://mlflow:5000
    depends_on:
      - backend
      - mlflow
    ports:
      - "8501:8501"
